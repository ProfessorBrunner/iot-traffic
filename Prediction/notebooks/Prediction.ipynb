{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prediction for IoT Traffic Simulation\n",
    "\n",
    "###Author: Neeraj Asthana and Professor Robert Brunner\n",
    "\n",
    "###Collaborators: Anchal, Rishabh, Vaishali\n",
    "\n",
    "___\n",
    "\n",
    "##Problem Statement\n",
    "Recent advances in sensor technology, infrastructure for estimating temperature, pressure, humidity, precipitation, etc. are readily available on roadways to predict traffic states. The goal and novelty of our project is to combine streamed GPS traffic estimation algorithms with dynamic road sensor data to accurately predict traffic states and efficiently advise and direct drivers.\n",
    "\n",
    "![alt text](car_road.png \"Cars and Sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cluster Details\n",
    "\n",
    "All of these resources are on the NCSA ACX cluster. \n",
    "\n",
    "###Spark Details\n",
    "\n",
    "\n",
    "###Kafka Details\n",
    "\n",
    "Nodes (brokers): 141.142.236.172, 141.142.236.194\n",
    "\n",
    "Port: 9092\n",
    "\n",
    "Zookeeper: 10.0.3.130:2181,10.0.3.131:2181\n",
    "\n",
    "###Cassandra Details\n",
    "\n",
    "Table name: traffic\n",
    "\n",
    "Schema:\n",
    "\n",
    "traffic (id uuid, time_stamp timestamp, latitude decimal, longitude decimal, PRIMARY KEY (id, time_stamp));\n",
    "\n",
    "____\n",
    "\n",
    "Visual of all resources:\n",
    "\n",
    "![alt text](Data Pipeline.png \"Data Pipeline\")\n",
    "\n",
    "Details of the cluster:\n",
    "\n",
    "![alt text](Cluster Details.png \"Cluster Details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Time Decay Model for Speed Estimation\n",
    "\n",
    "Used to estimate Average Road Speeds for specific segments of road by weighting speed observations.\n",
    "\n",
    "####Observations\n",
    "* Higher Speeds are more valuable the lower speeds\n",
    "* Recent Obersvations are favored over historical observations\n",
    "* Each data point will be of the form ($t_i, v_i, l_i$) or (time, velocity, location)\n",
    "\n",
    "\n",
    "####Time Weight\n",
    "An observation's timestamp ($t_i$) is weighted  by: $w(i,t) = \\frac{f(t_i - L)}{f(t - L)}$\n",
    "\n",
    "*f* is some positive, monotonic, non-decreasing function\n",
    "\n",
    "*L* is some Landmark time (starting time)\n",
    "\n",
    "*t* is the most recent timestamp\n",
    "\n",
    "####Velocity Weight\n",
    "An observation's velocity ($v_i$) is weighted  by: $w^v (i) = g (v_i)$\n",
    "\n",
    "*g* is some positive, monotonic, non-decreasing function\n",
    "\n",
    "####Combination of Weights\n",
    "Weight combinations of the time and velocity weights:\n",
    "\n",
    "$w^* (i,t) = w(i,t) \\cdot w^v (i) = \\frac{f(t_i - L)}{f(t - L)} \\cdot g(v_i)$\n",
    "\n",
    "####Aggregating observatoins\n",
    "Calculates average velocity of a road segment (*r*) by aggregating most recent and historical observations. In order compute these values efficiently and to be able to update values, I will persist 2 quantities in Spark , $X,Y$ which will be then be used to calulate $\\overline{V}(r) = \\frac{X}{Y}$\n",
    "\n",
    "$$X = \\sum_{n=1}^{m} f(t_i - L) \\cdot g(v_i) \\cdot v_i$$         \n",
    "\n",
    "$$Y = \\sum_{n=1}^{m} f(t_i - L) \\cdot g(v_i)$$\n",
    "\n",
    "Visual of the Time Decay Model:\n",
    "\n",
    "![alt text](Time Decay Visual.png \"Time Decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f():\n",
    "    None\n",
    "\n",
    "def g():\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spark Kafka Integration\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Write file with kafka script (use ipython to write script)\n",
    "\n",
    "2. use spark-submit to submit file with appropriate jars to Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing timedecaykafka.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile timedecaykafka.py\n",
    "\n",
    "##Spark Kafka\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# create SparkContext\n",
    "conf = SparkConf().setAppName(\"Kafka-Spark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# create StreamingContext (updates every 30 seconds)\n",
    "ssc = StreamingContext(sc, 30)\n",
    "\n",
    "topic = [\"mytopic\"]\n",
    "brokers =  \"141.142.236.172:9092,141.142.236.194:9092\"\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, topic, {\"metadata.broker.list\": brokers})\n",
    "\n",
    "lines = directKafkaStream.map(lambda x: x[1])\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!../../../spark/spark-1.5.0-bin-hadoop2.6/bin/spark-submit --master spark://10.0.3.70:7077 --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 timedecaykafka.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
