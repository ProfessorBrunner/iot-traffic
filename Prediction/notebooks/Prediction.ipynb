{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prediction for IoT Traffic Simulation\n",
    "\n",
    "###Author: Neeraj Asthana and Professor Robert Brunner\n",
    "\n",
    "###Collaborators: Anchal, Rishabh, Vaishali\n",
    "\n",
    "___\n",
    "\n",
    "##Problem Statement\n",
    "Recent advances in sensor technology, infrastructure for estimating temperature, pressure, humidity, precipitation, etc. are readily available on roadways to predict traffic states. The goal and novelty of our project is to combine streamed GPS traffic estimation algorithms with dynamic road sensor data to accurately predict traffic states and efficiently advise and direct drivers.\n",
    "\n",
    "![alt text](car_road.png \"Cars and Sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cluster Details\n",
    "\n",
    "All of these resources are on the NCSA ACX cluster. \n",
    "\n",
    "###Spark Details\n",
    "\n",
    "\n",
    "###Kafka Details\n",
    "\n",
    "Nodes (brokers): 141.142.236.172, 141.142.236.194\n",
    "\n",
    "Port: 9092\n",
    "\n",
    "Zookeeper: 10.0.3.130:2181,10.0.3.131:2181\n",
    "\n",
    "###Cassandra Details\n",
    "\n",
    "Table name: traffic\n",
    "\n",
    "Schema:\n",
    "\n",
    "traffic (id uuid, time_stamp timestamp, latitude decimal, longitude decimal, PRIMARY KEY (id, time_stamp));\n",
    "\n",
    "____\n",
    "\n",
    "Visual of all resources:\n",
    "\n",
    "![alt text](Data Pipeline.png \"Data Pipeline\")\n",
    "\n",
    "Details of the cluster:\n",
    "\n",
    "![alt text](Cluster Details.png \"Cluster Details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Time Decay Model for Speed Estimation\n",
    "\n",
    "Used to estimate Average Road Speeds for specific segments of road by weighting speed observations.\n",
    "\n",
    "####Observations\n",
    "* Higher Speeds are more valuable the lower speeds\n",
    "* Recent Obersvations are favored over historical observations\n",
    "* Each data point will be of the form ($t_i, v_i, l_i$) or (time, velocity, location)\n",
    "\n",
    "\n",
    "####Time Weight\n",
    "An observation's timestamp ($t_i$) is weighted  by: $w(i,t) = \\frac{f(t_i - L)}{f(t - L)}$\n",
    "\n",
    "*f* is some positive, monotonic, non-decreasing function\n",
    "\n",
    "*L* is some Landmark time (starting time)\n",
    "\n",
    "*t* is the most recent timestamp\n",
    "\n",
    "####Velocity Weight\n",
    "An observation's velocity ($v_i$) is weighted  by: $w^v (i) = g (v_i)$\n",
    "\n",
    "*g* is some positive, monotonic, non-decreasing function\n",
    "\n",
    "####Combination of Weights\n",
    "Weight combinations of the time and velocity weights:\n",
    "\n",
    "$w^* (i,t) = w(i,t) \\cdot w^v (i) = \\frac{f(t_i - L)}{f(t - L)} \\cdot g(v_i)$\n",
    "\n",
    "####Aggregating observatoins\n",
    "Calculates average velocity of a road segment (*r*) by aggregating most recent and historical observations. In order compute these values efficiently and to be able to update values, I will persist 2 quantities in Spark , $X,Y$ which will be then be used to calulate $\\overline{V}(r) = \\frac{X}{Y}$\n",
    "\n",
    "$$X = \\sum_{n=1}^{m} f(t_i - L) \\cdot g(v_i) \\cdot v_i$$         \n",
    "\n",
    "$$Y = \\sum_{n=1}^{m} f(t_i - L) \\cdot g(v_i)$$\n",
    "\n",
    "Visual of the Time Decay Model:\n",
    "\n",
    "![alt text](Time Decay Visual.png \"Time Decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Road Segments\n",
    "\n",
    "class Segment:\n",
    "    def __init__(self, latitude, longitude, landmark_time, identification):\n",
    "        self.latitude = latitude\n",
    "        self.longitude = longitude\n",
    "        self.landmark = landmark_time\n",
    "        self.current_time = landmark_time\n",
    "        self.id = identification\n",
    "        self.obs = 0\n",
    "        self.X = 0\n",
    "        self.Y = 0\n",
    "        self.speed = 0\n",
    "        \n",
    "segments = [Segment(47.1,47.1,0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Dynamic Map Matching Algorithm\n",
    "import math\n",
    "\n",
    "#Distance in kilometers between two latitude and longitude locations\n",
    "def distance(obsLoc, segmentLoc):\n",
    "    lat1, lon1 = obsLoc\n",
    "    lat2, lon2 = segmentLoc\n",
    "    radius = 6371 # km\n",
    "\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = radius * c\n",
    "\n",
    "    return d\n",
    "\n",
    "#match an observation to a road segment (within .25 km of a road segment)\n",
    "def findSegment(latitude, longitude):\n",
    "    obsLoc = (latitude,longitude)\n",
    "    \n",
    "    closest = None\n",
    "    for segment in segments:\n",
    "        segmentLoc = (segment.latitude, segment.longitude)\n",
    "        dist = distance(obsLoc, segmentLoc)\n",
    "        if closest == None or dist <= closest:\n",
    "            if dist <= .25:\n",
    "                closest = segment\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Time Decay Model\n",
    "\n",
    "def f(time):\n",
    "    return time\n",
    "\n",
    "def g(velocity):\n",
    "    return (velocity ** 2)\n",
    "\n",
    "def update(latitude, longitude, velocity, time):\n",
    "    #find segment using dynamic map matching algorithm\n",
    "    seg = findSegment(latitude,longitude)\n",
    "    \n",
    "    #weight time and velocity\n",
    "    wtime = f(time-self.landmark)\n",
    "    wvel = g(velocity)\n",
    "    \n",
    "    #produce X and Y for this observation and add these to the segment speed\n",
    "    X = wtime*wvel*velocity\n",
    "    Y = wtime*wvel\n",
    "    seg.X += X\n",
    "    seg.Y += Y\n",
    "    seg.speed = seg.X/seg.Y\n",
    "    seg.obs += 1\n",
    "    seg.current_time = time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spark Kafka Integration\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Write file with kafka script (use ipython to write script)\n",
    "\n",
    "2. use spark-submit to submit file with appropriate jars to Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timedecaykafka.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile timedecaykafka.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# create SparkContext\n",
    "conf = SparkConf().setAppName(\"Kafka-Spark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# create StreamingContext\n",
    "ssc = StreamingContext(sc, 30)\n",
    "\n",
    "# new approach (w/o receivers)\n",
    "topic = [\"mytopic\"]\n",
    "brokers =  \"141.142.236.172:9092,141.142.236.194:9092\"\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, topic, {\"metadata.broker.list\": brokers})\n",
    "\n",
    "lines = directKafkaStream.map(lambda x: x[1])\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "print(\"finished this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-streaming-kafka_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka_2.10;1.5.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.10;0.8.2.1 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.1.7 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka_2.10/1.5.0/spark-streaming-kafka_2.10-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka_2.10;1.5.0!spark-streaming-kafka_2.10.jar (42ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.2.1/kafka_2.10-0.8.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.10;0.8.2.1!kafka_2.10.jar (212ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
      "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.10!slf4j-api.jar (11ms)\n",
      "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.3.0!lz4.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.1.7!snappy-java.jar(bundle) (40ms)\n",
      "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
      "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (51ms)\n",
      ":: resolution report :: resolve 4373ms :: artifacts dl 520ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.10;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka_2.10;1.5.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.1.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   10  |   10  |   0   ||   10  |   10  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t10 artifacts copied, 0 already retrieved (5944kB/83ms)\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/04/22 17:17:25 INFO SparkContext: Running Spark version 1.5.0\n",
      "16/04/22 17:17:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/04/22 17:17:26 WARN Utils: Your hostname, neeraj-ipython resolves to a loopback address: 127.0.0.1; using 10.0.3.142 instead (on interface eth0)\n",
      "16/04/22 17:17:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "16/04/22 17:17:26 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "16/04/22 17:17:26 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "16/04/22 17:17:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ubuntu); users with modify permissions: Set(ubuntu)\n",
      "16/04/22 17:17:27 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/04/22 17:17:27 INFO Remoting: Starting remoting\n",
      "16/04/22 17:17:28 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.3.142:60969]\n",
      "16/04/22 17:17:28 INFO Utils: Successfully started service 'sparkDriver' on port 60969.\n",
      "16/04/22 17:17:28 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/04/22 17:17:28 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/04/22 17:17:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ce4ff06-6a53-44a9-b4c7-ef3e3b073e69\n",
      "16/04/22 17:17:28 INFO MemoryStore: MemoryStore started with capacity 530.3 MB\n",
      "16/04/22 17:17:28 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/httpd-b55c4a5d-156d-4945-8b2e-25fd8c66cfe3\n",
      "16/04/22 17:17:28 INFO HttpServer: Starting HTTP Server\n",
      "16/04/22 17:17:28 INFO Utils: Successfully started service 'HTTP file server' on port 40960.\n",
      "16/04/22 17:17:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/04/22 17:17:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/04/22 17:17:28 INFO SparkUI: Started SparkUI at http://10.0.3.142:4040\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar at http://10.0.3.142:40960/jars/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar with timestamp 1461345448937\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.10-0.8.2.1.jar at http://10.0.3.142:40960/jars/org.apache.kafka_kafka_2.10-0.8.2.1.jar with timestamp 1461345448967\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at http://10.0.3.142:40960/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1461345448968\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at http://10.0.3.142:40960/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1461345448970\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at http://10.0.3.142:40960/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1461345448973\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar at http://10.0.3.142:40960/jars/com.101tec_zkclient-0.3.jar with timestamp 1461345448974\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar at http://10.0.3.142:40960/jars/org.slf4j_slf4j-api-1.7.10.jar with timestamp 1461345448976\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at http://10.0.3.142:40960/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1461345448978\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.1.7.jar at http://10.0.3.142:40960/jars/org.xerial.snappy_snappy-java-1.1.1.7.jar with timestamp 1461345448987\n",
      "16/04/22 17:17:28 INFO SparkContext: Added JAR file:/home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar at http://10.0.3.142:40960/jars/log4j_log4j-1.2.17.jar with timestamp 1461345448991\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/iot-traffic/Prediction/notebooks/timedecaykafka.py to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/timedecaykafka.py\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/iot-traffic/Prediction/notebooks/timedecaykafka.py at http://10.0.3.142:40960/files/timedecaykafka.py with timestamp 1461345449162\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar at http://10.0.3.142:40960/files/org.apache.spark_spark-streaming-kafka_2.10-1.5.0.jar with timestamp 1461345449194\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.10-0.8.2.1.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.apache.kafka_kafka_2.10-0.8.2.1.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.10-0.8.2.1.jar at http://10.0.3.142:40960/files/org.apache.kafka_kafka_2.10-0.8.2.1.jar with timestamp 1461345449237\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.spark-project.spark_unused-1.0.0.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at http://10.0.3.142:40960/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1461345449286\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/com.yammer.metrics_metrics-core-2.2.0.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at http://10.0.3.142:40960/files/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1461345449295\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at http://10.0.3.142:40960/files/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1461345449313\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/com.101tec_zkclient-0.3.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar at http://10.0.3.142:40960/files/com.101tec_zkclient-0.3.jar with timestamp 1461345449329\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.slf4j_slf4j-api-1.7.10.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar at http://10.0.3.142:40960/files/org.slf4j_slf4j-api-1.7.10.jar with timestamp 1461345449346\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/net.jpountz.lz4_lz4-1.3.0.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at http://10.0.3.142:40960/files/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1461345449361\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.1.7.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/org.xerial.snappy_snappy-java-1.1.1.7.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.1.7.jar at http://10.0.3.142:40960/files/org.xerial.snappy_snappy-java-1.1.1.7.jar with timestamp 1461345449383\n",
      "16/04/22 17:17:29 INFO Utils: Copying /home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-ae01ce36-132f-41a6-abc3-50624d8fe1b0/userFiles-b2cbd87c-592b-4f29-abfd-06c973505283/log4j_log4j-1.2.17.jar\n",
      "16/04/22 17:17:29 INFO SparkContext: Added file file:/home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar at http://10.0.3.142:40960/files/log4j_log4j-1.2.17.jar with timestamp 1461345449401\n",
      "16/04/22 17:17:29 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.\n",
      "16/04/22 17:17:29 INFO AppClient$ClientEndpoint: Connecting to master spark://10.0.3.70:7077...\n",
      "16/04/22 17:17:30 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160422171730-0072\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor added: app-20160422171730-0072/0 on worker-20150930055354-10.0.3.108-41167 (10.0.3.108:41167) with 1 cores\n",
      "16/04/22 17:17:30 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160422171730-0072/0 on hostPort 10.0.3.108:41167 with 1 cores, 1024.0 MB RAM\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor added: app-20160422171730-0072/1 on worker-20160406235651-10.0.3.100-48441 (10.0.3.100:48441) with 1 cores\n",
      "16/04/22 17:17:30 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160422171730-0072/1 on hostPort 10.0.3.100:48441 with 1 cores, 1024.0 MB RAM\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor updated: app-20160422171730-0072/0 is now LOADING\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor updated: app-20160422171730-0072/1 is now LOADING\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor updated: app-20160422171730-0072/0 is now RUNNING\n",
      "16/04/22 17:17:30 INFO AppClient$ClientEndpoint: Executor updated: app-20160422171730-0072/1 is now RUNNING\n",
      "16/04/22 17:17:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45405.\n",
      "16/04/22 17:17:30 INFO NettyBlockTransferService: Server created on 45405\n",
      "16/04/22 17:17:30 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/04/22 17:17:30 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.3.142:45405 with 530.3 MB RAM, BlockManagerId(driver, 10.0.3.142, 45405)\n",
      "16/04/22 17:17:30 INFO BlockManagerMaster: Registered BlockManager\n",
      "16/04/22 17:17:30 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "16/04/22 17:17:31 INFO VerifiableProperties: Verifying properties\n",
      "16/04/22 17:17:31 INFO VerifiableProperties: Property group.id is overridden to \n",
      "16/04/22 17:17:31 INFO VerifiableProperties: Property zookeeper.connect is overridden to \n",
      "16/04/22 17:17:32 INFO ForEachDStream: metadataCleanupDelay = -1\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: metadataCleanupDelay = -1\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: Slide time = 30000 ms\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: Checkpoint interval = null\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: Remember duration = 30000 ms\n",
      "16/04/22 17:17:32 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@58401e00\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Slide time = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Remember duration = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@5971bf43\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Slide time = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Remember duration = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@3ac3f595\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Slide time = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Remember duration = 30000 ms\n",
      "16/04/22 17:17:32 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@17302757\n",
      "16/04/22 17:17:32 INFO ForEachDStream: Slide time = 30000 ms\n",
      "16/04/22 17:17:32 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "16/04/22 17:17:32 INFO ForEachDStream: Checkpoint interval = null\n",
      "16/04/22 17:17:32 INFO ForEachDStream: Remember duration = 30000 ms\n",
      "16/04/22 17:17:32 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5a66cca5\n",
      "16/04/22 17:17:32 INFO RecurringTimer: Started timer for JobGenerator at time 1461345480000\n",
      "16/04/22 17:17:32 INFO JobGenerator: Started JobGenerator at 1461345480000 ms\n",
      "16/04/22 17:17:32 INFO JobScheduler: Started JobScheduler\n",
      "16/04/22 17:17:32 INFO StreamingContext: StreamingContext started\n",
      "16/04/22 17:17:37 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@10.0.3.100:40626/user/Executor#-440325507]) with ID 1\n",
      "16/04/22 17:17:37 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@10.0.3.108:54340/user/Executor#1514527238]) with ID 0\n",
      "16/04/22 17:17:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.3.108:57281 with 530.3 MB RAM, BlockManagerId(0, 10.0.3.108, 57281)\n",
      "16/04/22 17:17:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.3.100:52291 with 530.3 MB RAM, BlockManagerId(1, 10.0.3.100, 52291)\n",
      "16/04/22 17:18:00 INFO VerifiableProperties: Verifying properties\n",
      "16/04/22 17:18:00 INFO VerifiableProperties: Property group.id is overridden to \n",
      "16/04/22 17:18:00 INFO VerifiableProperties: Property zookeeper.connect is overridden to \n",
      "16/04/22 17:18:00 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:18:03 INFO JobScheduler: Added jobs for time 1461345480000 ms\n",
      "16/04/22 17:18:03 INFO JobScheduler: Starting job streaming job 1461345480000 ms.0 from job set of time 1461345480000 ms\n",
      "16/04/22 17:18:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Registering RDD 4 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Final stage: ResultStage 1(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[4] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:18:04 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=0, maxMem=556038881\n",
      "16/04/22 17:18:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.7 KB, free 530.3 MB)\n",
      "16/04/22 17:18:04 INFO MemoryStore: ensureFreeSpace(6054) called with curMem=11976, maxMem=556038881\n",
      "16/04/22 17:18:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.3 MB)\n",
      "16/04/22 17:18:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[4] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:18:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "16/04/22 17:18:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:18:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3960 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:18:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:18:08 INFO DAGScheduler: ShuffleMapStage 0 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 4.004 s\n",
      "16/04/22 17:18:08 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:18:08 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:18:08 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "16/04/22 17:18:08 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:18:08 INFO DAGScheduler: Missing parents for ResultStage 1: List()\n",
      "16/04/22 17:18:08 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:18:08 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=18030, maxMem=556038881\n",
      "16/04/22 17:18:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.7 KB, free 530.3 MB)\n",
      "16/04/22 17:18:08 INFO MemoryStore: ensureFreeSpace(3732) called with curMem=24886, maxMem=556038881\n",
      "16/04/22 17:18:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.3 MB)\n",
      "16/04/22 17:18:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.3.142:45405 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:18:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "16/04/22 17:18:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:18:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.3.100:52291 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.3.100:40626\n",
      "16/04/22 17:18:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 139 bytes\n",
      "16/04/22 17:18:08 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:361) finished in 0.397 s\n",
      "16/04/22 17:18:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 391 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:18:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:18:08 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:361, took 4.857788 s\n",
      "16/04/22 17:18:08 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Final stage: ResultStage 3(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[9] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:18:09 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=28618, maxMem=556038881\n",
      "16/04/22 17:18:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:18:09 INFO MemoryStore: ensureFreeSpace(3732) called with curMem=35474, maxMem=556038881\n",
      "16/04/22 17:18:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.2 MB)\n",
      "16/04/22 17:18:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.3.142:45405 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[9] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:18:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\n",
      "16/04/22 17:18:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:18:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.3.108:57281 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.3.108:54340\n",
      "16/04/22 17:18:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 3348 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:18:12 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:361) finished in 3.351 s\n",
      "16/04/22 17:18:12 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:361, took 3.407883 s\n",
      "16/04/22 17:18:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:18:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:18:12 INFO JobScheduler: Finished job streaming job 1461345480000 ms.0 from job set of time 1461345480000 ms\n",
      "16/04/22 17:18:12 INFO JobScheduler: Total delay: 12.424 s for time 1461345480000 ms (execution: 8.428 s)\n",
      "16/04/22 17:18:12 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:18:12 INFO InputInfoTracker: remove old batch metadata: \n",
      "16/04/22 17:18:30 INFO JobScheduler: Added jobs for time 1461345510000 ms\n",
      "16/04/22 17:18:30 INFO JobScheduler: Starting job streaming job 1461345510000 ms.0 from job set of time 1461345510000 ms\n",
      "16/04/22 17:18:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Registering RDD 14 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Final stage: ResultStage 5(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[14] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:18:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=39206, maxMem=556038881\n",
      "16/04/22 17:18:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.7 KB, free 530.2 MB)\n",
      "16/04/22 17:18:30 INFO MemoryStore: ensureFreeSpace(6056) called with curMem=51182, maxMem=556038881\n",
      "16/04/22 17:18:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.2 MB)\n",
      "16/04/22 17:18:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (PairwiseRDD[14] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:18:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "16/04/22 17:18:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:18:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 389 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:18:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:18:30 INFO DAGScheduler: ShuffleMapStage 4 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.393 s\n",
      "16/04/22 17:18:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:18:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:18:30 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "16/04/22 17:18:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Missing parents for ResultStage 5: List()\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[18] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:18:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=57238, maxMem=556038881\n",
      "16/04/22 17:18:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:18:30 INFO MemoryStore: ensureFreeSpace(3735) called with curMem=64094, maxMem=556038881\n",
      "16/04/22 17:18:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.2 MB)\n",
      "16/04/22 17:18:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.3.142:45405 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[18] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:18:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "16/04/22 17:18:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:18:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.3.108:57281 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.3.108:54340\n",
      "16/04/22 17:18:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 139 bytes\n",
      "16/04/22 17:18:30 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 223 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:18:30 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:18:30 INFO DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:361) finished in 0.225 s\n",
      "16/04/22 17:18:30 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:361, took 0.727626 s\n",
      "16/04/22 17:18:31 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Final stage: ResultStage 7(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[19] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:18:31 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=67829, maxMem=556038881\n",
      "16/04/22 17:18:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:18:31 INFO MemoryStore: ensureFreeSpace(3735) called with curMem=74685, maxMem=556038881\n",
      "16/04/22 17:18:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.2 MB)\n",
      "16/04/22 17:18:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.3.142:45405 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:18:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD[19] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:18:31 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks\n",
      "16/04/22 17:18:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:18:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.3.108:57281 (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:18:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 175 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:18:31 INFO DAGScheduler: ResultStage 7 (runJob at PythonRDD.scala:361) finished in 0.178 s\n",
      "16/04/22 17:18:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:18:31 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:361, took 0.227799 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:18:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:18:31 INFO JobScheduler: Finished job streaming job 1461345510000 ms.0 from job set of time 1461345510000 ms\n",
      "16/04/22 17:18:31 INFO JobScheduler: Total delay: 1.264 s for time 1461345510000 ms (execution: 1.048 s)\n",
      "16/04/22 17:18:31 INFO PythonRDD: Removing RDD 7 from persistence list\n",
      "16/04/22 17:18:31 INFO BlockManager: Removing RDD 7\n",
      "16/04/22 17:18:31 INFO PythonRDD: Removing RDD 2 from persistence list\n",
      "16/04/22 17:18:31 INFO PythonRDD: Removing RDD 1 from persistence list\n",
      "16/04/22 17:18:31 INFO BlockManager: Removing RDD 2\n",
      "16/04/22 17:18:31 INFO BlockManager: Removing RDD 1\n",
      "16/04/22 17:18:31 INFO KafkaRDD: Removing RDD 0 from persistence list\n",
      "16/04/22 17:18:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:18:31 INFO InputInfoTracker: remove old batch metadata: \n",
      "16/04/22 17:18:31 INFO BlockManager: Removing RDD 0\n",
      "16/04/22 17:19:00 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:19:04 INFO JobScheduler: Added jobs for time 1461345540000 ms\n",
      "16/04/22 17:19:04 INFO JobScheduler: Starting job streaming job 1461345540000 ms.0 from job set of time 1461345540000 ms\n",
      "16/04/22 17:19:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.3.142:45405 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Registering RDD 24 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Final stage: ResultStage 9(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[24] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=67829, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=79805, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.2 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (PairwiseRDD[24] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:19:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.3.108:57281 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.3.142:45405 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.3.108:57281 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 7\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 6\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.3.142:45405 in memory (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.3.108:57281 in memory (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 5\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.3.142:45405 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.3.108:57281 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 4\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.3.142:45405 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.3.100:52291 in memory (size: 3.6 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 3\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.3.142:45405 in memory (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.3.100:52291 in memory (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned accumulator 2\n",
      "16/04/22 17:19:04 INFO ContextCleaner: Cleaned shuffle 0\n",
      "16/04/22 17:19:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 361 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:19:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:19:04 INFO DAGScheduler: ShuffleMapStage 8 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.365 s\n",
      "16/04/22 17:19:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:19:04 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:19:04 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Missing parents for ResultStage 9: List()\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[28] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=18034, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.7 KB, free 530.3 MB)\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=24890, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.3 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[28] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:19:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "16/04/22 17:19:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.3.100:40626\n",
      "16/04/22 17:19:04 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 139 bytes\n",
      "16/04/22 17:19:04 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 271 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:19:04 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:19:04 INFO DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:361) finished in 0.276 s\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:361, took 0.739734 s\n",
      "16/04/22 17:19:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Final stage: ResultStage 11(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[29] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=28631, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:04 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=35487, maxMem=556038881\n",
      "16/04/22 17:19:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:04 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[29] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:19:04 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks\n",
      "16/04/22 17:19:04 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:19:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.3.108:54340\n",
      "16/04/22 17:19:05 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 195 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:19:05 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:361) finished in 0.198 s\n",
      "16/04/22 17:19:05 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:361, took 0.259979 s\n",
      "16/04/22 17:19:05 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:19:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:19:05 INFO JobScheduler: Finished job streaming job 1461345540000 ms.0 from job set of time 1461345540000 ms\n",
      "16/04/22 17:19:05 INFO JobScheduler: Total delay: 5.211 s for time 1461345540000 ms (execution: 1.131 s)\n",
      "16/04/22 17:19:05 INFO PythonRDD: Removing RDD 17 from persistence list\n",
      "16/04/22 17:19:05 INFO BlockManager: Removing RDD 17\n",
      "16/04/22 17:19:05 INFO PythonRDD: Removing RDD 12 from persistence list\n",
      "16/04/22 17:19:05 INFO BlockManager: Removing RDD 12\n",
      "16/04/22 17:19:05 INFO PythonRDD: Removing RDD 11 from persistence list\n",
      "16/04/22 17:19:05 INFO BlockManager: Removing RDD 11\n",
      "16/04/22 17:19:05 INFO KafkaRDD: Removing RDD 10 from persistence list\n",
      "16/04/22 17:19:05 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:19:05 INFO InputInfoTracker: remove old batch metadata: 1461345480000 ms\n",
      "16/04/22 17:19:05 INFO BlockManager: Removing RDD 10\n",
      "16/04/22 17:19:30 INFO JobScheduler: Added jobs for time 1461345570000 ms\n",
      "16/04/22 17:19:30 INFO JobScheduler: Starting job streaming job 1461345570000 ms.0 from job set of time 1461345570000 ms\n",
      "16/04/22 17:19:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Registering RDD 34 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Final stage: ResultStage 13(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[34] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=39228, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(6059) called with curMem=51204, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.2 MB)\n",
      "16/04/22 17:19:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (PairwiseRDD[34] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:19:30 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks\n",
      "16/04/22 17:19:30 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:19:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:30 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 358 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:19:30 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:19:30 INFO DAGScheduler: ShuffleMapStage 12 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.364 s\n",
      "16/04/22 17:19:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:19:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:19:30 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Missing parents for ResultStage 13: List()\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[38] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=57263, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(3740) called with curMem=64119, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:30 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[38] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:19:30 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "16/04/22 17:19:30 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:19:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.3.100:40626\n",
      "16/04/22 17:19:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 139 bytes\n",
      "16/04/22 17:19:30 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 209 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:19:30 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:19:30 INFO DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:361) finished in 0.213 s\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:361, took 0.705757 s\n",
      "16/04/22 17:19:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Got job 7 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Final stage: ResultStage 15(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:19:30 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[39] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=67859, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:30 INFO MemoryStore: ensureFreeSpace(3740) called with curMem=74715, maxMem=556038881\n",
      "16/04/22 17:19:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.2 MB)\n",
      "16/04/22 17:19:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:31 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:19:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[39] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:19:31 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks\n",
      "16/04/22 17:19:31 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 11, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:19:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:19:31 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 11) in 193 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:19:31 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:19:31 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:361) finished in 0.199 s\n",
      "16/04/22 17:19:31 INFO DAGScheduler: Job 7 finished: runJob at PythonRDD.scala:361, took 0.235771 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:19:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:19:31 INFO JobScheduler: Finished job streaming job 1461345570000 ms.0 from job set of time 1461345570000 ms\n",
      "16/04/22 17:19:31 INFO JobScheduler: Total delay: 1.217 s for time 1461345570000 ms (execution: 1.050 s)\n",
      "16/04/22 17:19:31 INFO PythonRDD: Removing RDD 27 from persistence list\n",
      "16/04/22 17:19:31 INFO PythonRDD: Removing RDD 22 from persistence list\n",
      "16/04/22 17:19:31 INFO BlockManager: Removing RDD 27\n",
      "16/04/22 17:19:31 INFO BlockManager: Removing RDD 22\n",
      "16/04/22 17:19:31 INFO PythonRDD: Removing RDD 21 from persistence list\n",
      "16/04/22 17:19:31 INFO BlockManager: Removing RDD 21\n",
      "16/04/22 17:19:31 INFO KafkaRDD: Removing RDD 20 from persistence list\n",
      "16/04/22 17:19:31 INFO BlockManager: Removing RDD 20\n",
      "16/04/22 17:19:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:19:31 INFO InputInfoTracker: remove old batch metadata: 1461345510000 ms\n",
      "16/04/22 17:20:00 INFO JobScheduler: Added jobs for time 1461345600000 ms\n",
      "16/04/22 17:20:00 INFO JobScheduler: Starting job streaming job 1461345600000 ms.0 from job set of time 1461345600000 ms\n",
      "16/04/22 17:20:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Registering RDD 44 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Final stage: ResultStage 17(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting ShuffleMapStage 16 (PairwiseRDD[44] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=78455, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=90431, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (PairwiseRDD[44] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:20:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks\n",
      "16/04/22 17:20:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 12, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:20:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 12) in 269 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:20:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:00 INFO DAGScheduler: ShuffleMapStage 16 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.273 s\n",
      "16/04/22 17:20:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:20:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:20:00 INFO DAGScheduler: waiting: Set(ResultStage 17)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Missing parents for ResultStage 17: List()\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[48] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=96489, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=103345, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (PythonRDD[48] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:20:00 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks\n",
      "16/04/22 17:20:00 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:20:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.3.100:40626\n",
      "16/04/22 17:20:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 139 bytes\n",
      "16/04/22 17:20:00 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 303 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:20:00 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:00 INFO DAGScheduler: ResultStage 17 (runJob at PythonRDD.scala:361) finished in 0.309 s\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:361, took 0.701647 s\n",
      "16/04/22 17:20:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Final stage: ResultStage 19(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[49] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=107088, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 6.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=113944, maxMem=556038881\n",
      "16/04/22 17:20:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (PythonRDD[49] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:20:00 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks\n",
      "16/04/22 17:20:00 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:20:01 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.3.108:54340\n",
      "16/04/22 17:20:01 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 203 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:20:01 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:01 INFO DAGScheduler: ResultStage 19 (runJob at PythonRDD.scala:361) finished in 0.208 s\n",
      "16/04/22 17:20:01 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:361, took 0.263664 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:20:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:20:01 INFO JobScheduler: Finished job streaming job 1461345600000 ms.0 from job set of time 1461345600000 ms\n",
      "16/04/22 17:20:01 INFO PythonRDD: Removing RDD 37 from persistence list\n",
      "16/04/22 17:20:01 INFO JobScheduler: Total delay: 1.213 s for time 1461345600000 ms (execution: 1.068 s)\n",
      "16/04/22 17:20:01 INFO BlockManager: Removing RDD 37\n",
      "16/04/22 17:20:01 INFO PythonRDD: Removing RDD 32 from persistence list\n",
      "16/04/22 17:20:01 INFO PythonRDD: Removing RDD 31 from persistence list\n",
      "16/04/22 17:20:01 INFO BlockManager: Removing RDD 32\n",
      "16/04/22 17:20:01 INFO BlockManager: Removing RDD 31\n",
      "16/04/22 17:20:01 INFO KafkaRDD: Removing RDD 30 from persistence list\n",
      "16/04/22 17:20:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:20:01 INFO InputInfoTracker: remove old batch metadata: 1461345540000 ms\n",
      "16/04/22 17:20:01 INFO BlockManager: Removing RDD 30\n",
      "16/04/22 17:20:30 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:20:33 INFO JobScheduler: Added jobs for time 1461345630000 ms\n",
      "16/04/22 17:20:33 INFO JobScheduler: Starting job streaming job 1461345630000 ms.0 from job set of time 1461345630000 ms\n",
      "16/04/22 17:20:33 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Registering RDD 54 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Got job 10 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Final stage: ResultStage 21(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Submitting ShuffleMapStage 20 (PairwiseRDD[54] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:20:33 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=117687, maxMem=556038881\n",
      "16/04/22 17:20:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.7 KB, free 530.2 MB)\n",
      "16/04/22 17:20:33 INFO MemoryStore: ensureFreeSpace(6056) called with curMem=129663, maxMem=556038881\n",
      "16/04/22 17:20:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.2 MB)\n",
      "16/04/22 17:20:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:33 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (PairwiseRDD[54] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:20:33 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks\n",
      "16/04/22 17:20:33 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 15, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:20:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:34 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 15) in 248 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:20:34 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:34 INFO DAGScheduler: ShuffleMapStage 20 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.251 s\n",
      "16/04/22 17:20:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:20:34 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:20:34 INFO DAGScheduler: waiting: Set(ResultStage 21)\n",
      "16/04/22 17:20:34 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Missing parents for ResultStage 21: List()\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[58] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:20:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=135719, maxMem=556038881\n",
      "16/04/22 17:20:34 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:20:34 INFO MemoryStore: ensureFreeSpace(3740) called with curMem=142575, maxMem=556038881\n",
      "16/04/22 17:20:34 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:20:34 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:34 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (PythonRDD[58] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:20:34 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks\n",
      "16/04/22 17:20:34 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:20:34 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.3.100:40626\n",
      "16/04/22 17:20:34 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 139 bytes\n",
      "16/04/22 17:20:34 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 207 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:20:34 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:34 INFO DAGScheduler: ResultStage 21 (runJob at PythonRDD.scala:361) finished in 0.212 s\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:361, took 0.595064 s\n",
      "16/04/22 17:20:34 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Final stage: ResultStage 23(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[59] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:20:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=146315, maxMem=556038881\n",
      "16/04/22 17:20:34 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:20:34 INFO MemoryStore: ensureFreeSpace(3740) called with curMem=153171, maxMem=556038881\n",
      "16/04/22 17:20:34 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:20:34 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:20:34 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (PythonRDD[59] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:20:34 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks\n",
      "16/04/22 17:20:34 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 17, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:20:34 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.3 MB)\n",
      "16/04/22 17:20:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.3.108:54340\n",
      "16/04/22 17:20:34 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 17) in 202 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:20:34 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:20:34 INFO DAGScheduler: ResultStage 23 (runJob at PythonRDD.scala:361) finished in 0.207 s\n",
      "16/04/22 17:20:34 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:361, took 0.277659 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:20:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:20:34 INFO JobScheduler: Finished job streaming job 1461345630000 ms.0 from job set of time 1461345630000 ms\n",
      "16/04/22 17:20:34 INFO JobScheduler: Total delay: 4.694 s for time 1461345630000 ms (execution: 0.956 s)\n",
      "16/04/22 17:20:34 INFO PythonRDD: Removing RDD 47 from persistence list\n",
      "16/04/22 17:20:34 INFO BlockManager: Removing RDD 47\n",
      "16/04/22 17:20:34 INFO PythonRDD: Removing RDD 42 from persistence list\n",
      "16/04/22 17:20:34 INFO BlockManager: Removing RDD 42\n",
      "16/04/22 17:20:34 INFO PythonRDD: Removing RDD 41 from persistence list\n",
      "16/04/22 17:20:34 INFO BlockManager: Removing RDD 41\n",
      "16/04/22 17:20:34 INFO KafkaRDD: Removing RDD 40 from persistence list\n",
      "16/04/22 17:20:34 INFO BlockManager: Removing RDD 40\n",
      "16/04/22 17:20:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:20:34 INFO InputInfoTracker: remove old batch metadata: 1461345570000 ms\n",
      "16/04/22 17:21:00 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:21:03 INFO JobScheduler: Added jobs for time 1461345660000 ms\n",
      "16/04/22 17:21:03 INFO JobScheduler: Starting job streaming job 1461345660000 ms.0 from job set of time 1461345660000 ms\n",
      "16/04/22 17:21:03 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Registering RDD 64 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Got job 12 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Final stage: ResultStage 25(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Submitting ShuffleMapStage 24 (PairwiseRDD[64] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:21:03 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=156911, maxMem=556038881\n",
      "16/04/22 17:21:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 11.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:03 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=168887, maxMem=556038881\n",
      "16/04/22 17:21:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.1 MB)\n",
      "16/04/22 17:21:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:03 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (PairwiseRDD[64] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:21:03 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks\n",
      "16/04/22 17:21:03 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:21:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.3 MB)\n",
      "16/04/22 17:21:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 208 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:21:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:04 INFO DAGScheduler: ShuffleMapStage 24 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.210 s\n",
      "16/04/22 17:21:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:21:04 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:21:04 INFO DAGScheduler: waiting: Set(ResultStage 25)\n",
      "16/04/22 17:21:04 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Missing parents for ResultStage 25: List()\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[68] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:21:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=174945, maxMem=556038881\n",
      "16/04/22 17:21:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:04 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=181801, maxMem=556038881\n",
      "16/04/22 17:21:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[68] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:21:04 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks\n",
      "16/04/22 17:21:04 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:21:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.3.108:54340\n",
      "16/04/22 17:21:04 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 139 bytes\n",
      "16/04/22 17:21:04 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 187 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:21:04 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:04 INFO DAGScheduler: ResultStage 25 (runJob at PythonRDD.scala:361) finished in 0.190 s\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Job 12 finished: runJob at PythonRDD.scala:361, took 0.503777 s\n",
      "16/04/22 17:21:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Got job 13 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Final stage: ResultStage 27(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Submitting ResultStage 27 (PythonRDD[69] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:21:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=185543, maxMem=556038881\n",
      "16/04/22 17:21:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:04 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=192399, maxMem=556038881\n",
      "16/04/22 17:21:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (PythonRDD[69] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:21:04 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks\n",
      "16/04/22 17:21:04 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:21:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:04 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 185 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:21:04 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:04 INFO DAGScheduler: ResultStage 27 (runJob at PythonRDD.scala:361) finished in 0.187 s\n",
      "16/04/22 17:21:04 INFO DAGScheduler: Job 13 finished: runJob at PythonRDD.scala:361, took 0.233932 s\n",
      "16/04/22 17:21:04 INFO JobScheduler: Finished job streaming job 1461345660000 ms.0 from job set of time 1461345660000 ms\n",
      "16/04/22 17:21:04 INFO JobScheduler: Total delay: 4.554 s for time 1461345660000 ms (execution: 0.799 s)\n",
      "16/04/22 17:21:04 INFO PythonRDD: Removing RDD 57 from persistence list\n",
      "16/04/22 17:21:04 INFO PythonRDD: Removing RDD 52 from persistence list\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:21:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:21:04 INFO PythonRDD: Removing RDD 51 from persistence list\n",
      "16/04/22 17:21:04 INFO BlockManager: Removing RDD 52\n",
      "16/04/22 17:21:04 INFO BlockManager: Removing RDD 57\n",
      "16/04/22 17:21:04 INFO KafkaRDD: Removing RDD 50 from persistence list\n",
      "16/04/22 17:21:04 INFO BlockManager: Removing RDD 51\n",
      "16/04/22 17:21:04 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:21:04 INFO InputInfoTracker: remove old batch metadata: 1461345600000 ms\n",
      "16/04/22 17:21:04 INFO BlockManager: Removing RDD 50\n",
      "16/04/22 17:21:30 INFO JobScheduler: Starting job streaming job 1461345690000 ms.0 from job set of time 1461345690000 ms\n",
      "16/04/22 17:21:30 INFO JobScheduler: Added jobs for time 1461345690000 ms\n",
      "16/04/22 17:21:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Registering RDD 74 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Final stage: ResultStage 29(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting ShuffleMapStage 28 (PairwiseRDD[74] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=196141, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 11.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=208117, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:30 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (PairwiseRDD[74] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:21:30 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks\n",
      "16/04/22 17:21:30 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 21, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:30 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 21) in 219 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:21:30 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:30 INFO DAGScheduler: ShuffleMapStage 28 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.222 s\n",
      "16/04/22 17:21:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:21:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:21:30 INFO DAGScheduler: waiting: Set(ResultStage 29)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Missing parents for ResultStage 29: List()\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting ResultStage 29 (PythonRDD[78] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=214175, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=221031, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:30 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (PythonRDD[78] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:21:30 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks\n",
      "16/04/22 17:21:30 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 22, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.3.108:54340\n",
      "16/04/22 17:21:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 139 bytes\n",
      "16/04/22 17:21:30 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 22) in 203 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: ResultStage 29 (runJob at PythonRDD.scala:361) finished in 0.206 s\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:361, took 0.518660 s\n",
      "16/04/22 17:21:30 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Final stage: ResultStage 31(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting ResultStage 31 (PythonRDD[79] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=224773, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 6.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=231629, maxMem=556038881\n",
      "16/04/22 17:21:30 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.1 MB)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:30 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:21:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (PythonRDD[79] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:21:30 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks\n",
      "16/04/22 17:21:30 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 23, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:21:30 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:21:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.3.100:40626\n",
      "16/04/22 17:21:31 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 23) in 220 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:21:31 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:21:31 INFO DAGScheduler: ResultStage 31 (runJob at PythonRDD.scala:361) finished in 0.224 s\n",
      "16/04/22 17:21:31 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:361, took 0.282320 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:21:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:21:31 INFO JobScheduler: Finished job streaming job 1461345690000 ms.0 from job set of time 1461345690000 ms\n",
      "16/04/22 17:21:31 INFO JobScheduler: Total delay: 1.086 s for time 1461345690000 ms (execution: 0.924 s)\n",
      "16/04/22 17:21:31 INFO PythonRDD: Removing RDD 67 from persistence list\n",
      "16/04/22 17:21:31 INFO BlockManager: Removing RDD 67\n",
      "16/04/22 17:21:31 INFO PythonRDD: Removing RDD 62 from persistence list\n",
      "16/04/22 17:21:31 INFO BlockManager: Removing RDD 62\n",
      "16/04/22 17:21:31 INFO PythonRDD: Removing RDD 61 from persistence list\n",
      "16/04/22 17:21:31 INFO BlockManager: Removing RDD 61\n",
      "16/04/22 17:21:31 INFO KafkaRDD: Removing RDD 60 from persistence list\n",
      "16/04/22 17:21:31 INFO BlockManager: Removing RDD 60\n",
      "16/04/22 17:21:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:21:31 INFO InputInfoTracker: remove old batch metadata: 1461345630000 ms\n",
      "16/04/22 17:22:00 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:22:03 INFO JobScheduler: Added jobs for time 1461345720000 ms\n",
      "16/04/22 17:22:03 INFO JobScheduler: Starting job streaming job 1461345720000 ms.0 from job set of time 1461345720000 ms\n",
      "16/04/22 17:22:03 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Registering RDD 84 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Final stage: ResultStage 33(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Submitting ShuffleMapStage 32 (PairwiseRDD[84] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:22:03 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=235371, maxMem=556038881\n",
      "16/04/22 17:22:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 11.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:03 INFO MemoryStore: ensureFreeSpace(6054) called with curMem=247347, maxMem=556038881\n",
      "16/04/22 17:22:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.0 MB)\n",
      "16/04/22 17:22:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:03 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (PairwiseRDD[84] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:22:03 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks\n",
      "16/04/22 17:22:03 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 24, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:22:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:04 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 24) in 230 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:22:04 INFO DAGScheduler: ShuffleMapStage 32 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.235 s\n",
      "16/04/22 17:22:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:22:04 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:22:04 INFO DAGScheduler: waiting: Set(ResultStage 33)\n",
      "16/04/22 17:22:04 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:22:04 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:04 INFO DAGScheduler: Missing parents for ResultStage 33: List()\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Submitting ResultStage 33 (PythonRDD[88] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:22:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=253401, maxMem=556038881\n",
      "16/04/22 17:22:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 6.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:04 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=260257, maxMem=556038881\n",
      "16/04/22 17:22:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (PythonRDD[88] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:22:04 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks\n",
      "16/04/22 17:22:04 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 25, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:22:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.3.100:40626\n",
      "16/04/22 17:22:04 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 139 bytes\n",
      "16/04/22 17:22:04 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 25) in 181 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:22:04 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:04 INFO DAGScheduler: ResultStage 33 (runJob at PythonRDD.scala:361) finished in 0.185 s\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:361, took 0.513401 s\n",
      "16/04/22 17:22:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Got job 17 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Final stage: ResultStage 35(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[89] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:22:04 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=263999, maxMem=556038881\n",
      "16/04/22 17:22:04 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 6.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:04 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=270855, maxMem=556038881\n",
      "16/04/22 17:22:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:04 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (PythonRDD[89] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:22:04 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks\n",
      "16/04/22 17:22:04 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 26, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:22:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.3.108:54340\n",
      "16/04/22 17:22:04 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 26) in 199 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:22:04 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:04 INFO DAGScheduler: ResultStage 35 (runJob at PythonRDD.scala:361) finished in 0.200 s\n",
      "16/04/22 17:22:04 INFO DAGScheduler: Job 17 finished: runJob at PythonRDD.scala:361, took 0.273329 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:22:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:22:04 INFO JobScheduler: Finished job streaming job 1461345720000 ms.0 from job set of time 1461345720000 ms\n",
      "16/04/22 17:22:04 INFO JobScheduler: Total delay: 4.631 s for time 1461345720000 ms (execution: 0.877 s)\n",
      "16/04/22 17:22:04 INFO PythonRDD: Removing RDD 77 from persistence list\n",
      "16/04/22 17:22:04 INFO BlockManager: Removing RDD 77\n",
      "16/04/22 17:22:04 INFO PythonRDD: Removing RDD 72 from persistence list\n",
      "16/04/22 17:22:04 INFO BlockManager: Removing RDD 72\n",
      "16/04/22 17:22:04 INFO PythonRDD: Removing RDD 71 from persistence list\n",
      "16/04/22 17:22:04 INFO KafkaRDD: Removing RDD 70 from persistence list\n",
      "16/04/22 17:22:04 INFO BlockManager: Removing RDD 71\n",
      "16/04/22 17:22:04 INFO BlockManager: Removing RDD 70\n",
      "16/04/22 17:22:04 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:22:04 INFO InputInfoTracker: remove old batch metadata: 1461345660000 ms\n",
      "16/04/22 17:22:30 INFO JobScheduler: Added jobs for time 1461345750000 ms\n",
      "16/04/22 17:22:30 INFO JobScheduler: Starting job streaming job 1461345750000 ms.0 from job set of time 1461345750000 ms\n",
      "16/04/22 17:22:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Registering RDD 94 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Got job 18 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Final stage: ResultStage 37(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting ShuffleMapStage 36 (PairwiseRDD[94] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=274597, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 11.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=286573, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (PairwiseRDD[94] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:22:30 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks\n",
      "16/04/22 17:22:30 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 27, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 27) in 211 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:22:30 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:30 INFO DAGScheduler: ShuffleMapStage 36 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.214 s\n",
      "16/04/22 17:22:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:22:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:22:30 INFO DAGScheduler: waiting: Set(ResultStage 37)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Missing parents for ResultStage 37: List()\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting ResultStage 37 (PythonRDD[98] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=292631, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 6.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=299487, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (PythonRDD[98] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:22:30 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks\n",
      "16/04/22 17:22:30 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 28, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.3.100:40626\n",
      "16/04/22 17:22:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 139 bytes\n",
      "16/04/22 17:22:30 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 28) in 164 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:22:30 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:30 INFO DAGScheduler: ResultStage 37 (runJob at PythonRDD.scala:361) finished in 0.167 s\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Job 18 finished: runJob at PythonRDD.scala:361, took 0.495567 s\n",
      "16/04/22 17:22:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Got job 19 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Final stage: ResultStage 39(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting ResultStage 39 (PythonRDD[99] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=303228, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 6.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=310084, maxMem=556038881\n",
      "16/04/22 17:22:30 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.0 MB)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (PythonRDD[99] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:22:30 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks\n",
      "16/04/22 17:22:30 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:22:30 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:22:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.3.108:54340\n",
      "16/04/22 17:22:31 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 206 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:22:31 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:22:31 INFO DAGScheduler: ResultStage 39 (runJob at PythonRDD.scala:361) finished in 0.214 s\n",
      "16/04/22 17:22:31 INFO DAGScheduler: Job 19 finished: runJob at PythonRDD.scala:361, took 0.303411 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:22:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:22:31 INFO JobScheduler: Finished job streaming job 1461345750000 ms.0 from job set of time 1461345750000 ms\n",
      "16/04/22 17:22:31 INFO PythonRDD: Removing RDD 87 from persistence list\n",
      "16/04/22 17:22:31 INFO JobScheduler: Total delay: 1.031 s for time 1461345750000 ms (execution: 0.881 s)\n",
      "16/04/22 17:22:31 INFO PythonRDD: Removing RDD 82 from persistence list\n",
      "16/04/22 17:22:31 INFO BlockManager: Removing RDD 87\n",
      "16/04/22 17:22:31 INFO BlockManager: Removing RDD 82\n",
      "16/04/22 17:22:31 INFO PythonRDD: Removing RDD 81 from persistence list\n",
      "16/04/22 17:22:31 INFO KafkaRDD: Removing RDD 80 from persistence list\n",
      "16/04/22 17:22:31 INFO BlockManager: Removing RDD 81\n",
      "16/04/22 17:22:31 INFO BlockManager: Removing RDD 80\n",
      "16/04/22 17:22:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:22:31 INFO InputInfoTracker: remove old batch metadata: 1461345690000 ms\n",
      "16/04/22 17:23:00 INFO JobScheduler: Added jobs for time 1461345780000 ms\n",
      "16/04/22 17:23:00 INFO JobScheduler: Starting job streaming job 1461345780000 ms.0 from job set of time 1461345780000 ms\n",
      "16/04/22 17:23:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Registering RDD 104 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Got job 20 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Final stage: ResultStage 41(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 40)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting ShuffleMapStage 40 (PairwiseRDD[104] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=313825, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 11.7 KB, free 530.0 MB)\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(6055) called with curMem=325801, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.9 KB, free 530.0 MB)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:00 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (PairwiseRDD[104] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:23:00 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks\n",
      "16/04/22 17:23:00 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:00 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 207 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:23:00 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:00 INFO DAGScheduler: ShuffleMapStage 40 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.210 s\n",
      "16/04/22 17:23:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:23:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:23:00 INFO DAGScheduler: waiting: Set(ResultStage 41)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Missing parents for ResultStage 41: List()\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting ResultStage 41 (PythonRDD[108] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=331856, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 6.7 KB, free 530.0 MB)\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=338712, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.7 KB, free 530.0 MB)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:00 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (PythonRDD[108] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:23:00 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks\n",
      "16/04/22 17:23:00 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.3.108:54340\n",
      "16/04/22 17:23:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 139 bytes\n",
      "16/04/22 17:23:00 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 203 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:23:00 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:00 INFO DAGScheduler: ResultStage 41 (runJob at PythonRDD.scala:361) finished in 0.206 s\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Job 20 finished: runJob at PythonRDD.scala:361, took 0.540796 s\n",
      "16/04/22 17:23:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Got job 21 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Final stage: ResultStage 43(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting ResultStage 43 (PythonRDD[109] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=342454, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 6.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:00 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=349310, maxMem=556038881\n",
      "16/04/22 17:23:00 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:00 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (PythonRDD[109] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:23:00 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks\n",
      "16/04/22 17:23:00 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 32, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:23:00 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:01 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 32) in 209 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:23:01 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:01 INFO DAGScheduler: ResultStage 43 (runJob at PythonRDD.scala:361) finished in 0.214 s\n",
      "16/04/22 17:23:01 INFO DAGScheduler: Job 21 finished: runJob at PythonRDD.scala:361, took 0.273428 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:23:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:23:01 INFO JobScheduler: Finished job streaming job 1461345780000 ms.0 from job set of time 1461345780000 ms\n",
      "16/04/22 17:23:01 INFO JobScheduler: Total delay: 1.080 s for time 1461345780000 ms (execution: 0.927 s)\n",
      "16/04/22 17:23:01 INFO PythonRDD: Removing RDD 97 from persistence list\n",
      "16/04/22 17:23:01 INFO BlockManager: Removing RDD 97\n",
      "16/04/22 17:23:01 INFO PythonRDD: Removing RDD 92 from persistence list\n",
      "16/04/22 17:23:01 INFO BlockManager: Removing RDD 92\n",
      "16/04/22 17:23:01 INFO PythonRDD: Removing RDD 91 from persistence list\n",
      "16/04/22 17:23:01 INFO BlockManager: Removing RDD 91\n",
      "16/04/22 17:23:01 INFO KafkaRDD: Removing RDD 90 from persistence list\n",
      "16/04/22 17:23:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:23:01 INFO InputInfoTracker: remove old batch metadata: 1461345720000 ms\n",
      "16/04/22 17:23:01 INFO BlockManager: Removing RDD 90\n",
      "16/04/22 17:23:30 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:23:33 INFO JobScheduler: Added jobs for time 1461345810000 ms\n",
      "16/04/22 17:23:33 INFO JobScheduler: Starting job streaming job 1461345810000 ms.0 from job set of time 1461345810000 ms\n",
      "16/04/22 17:23:33 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Registering RDD 114 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Got job 22 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Final stage: ResultStage 45(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Submitting ShuffleMapStage 44 (PairwiseRDD[114] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:23:33 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=353052, maxMem=556038881\n",
      "16/04/22 17:23:33 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:33 INFO MemoryStore: ensureFreeSpace(6057) called with curMem=365028, maxMem=556038881\n",
      "16/04/22 17:23:33 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.9 MB)\n",
      "16/04/22 17:23:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:33 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 44 (PairwiseRDD[114] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:23:33 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks\n",
      "16/04/22 17:23:33 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 33, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:23:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:34 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 33) in 274 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:23:34 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:34 INFO DAGScheduler: ShuffleMapStage 44 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.281 s\n",
      "16/04/22 17:23:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:23:34 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:23:34 INFO DAGScheduler: waiting: Set(ResultStage 45)\n",
      "16/04/22 17:23:34 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Missing parents for ResultStage 45: List()\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Submitting ResultStage 45 (PythonRDD[118] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:23:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=371085, maxMem=556038881\n",
      "16/04/22 17:23:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 6.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:34 INFO MemoryStore: ensureFreeSpace(3738) called with curMem=377941, maxMem=556038881\n",
      "16/04/22 17:23:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:34 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (PythonRDD[118] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:23:34 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks\n",
      "16/04/22 17:23:34 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 34, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:23:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.3.100:40626\n",
      "16/04/22 17:23:34 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 139 bytes\n",
      "16/04/22 17:23:34 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 34) in 164 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:23:34 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:34 INFO DAGScheduler: ResultStage 45 (runJob at PythonRDD.scala:361) finished in 0.168 s\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Job 22 finished: runJob at PythonRDD.scala:361, took 0.550256 s\n",
      "16/04/22 17:23:34 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Final stage: ResultStage 47(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Submitting ResultStage 47 (PythonRDD[119] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:23:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=381679, maxMem=556038881\n",
      "16/04/22 17:23:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 6.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:34 INFO MemoryStore: ensureFreeSpace(3738) called with curMem=388535, maxMem=556038881\n",
      "16/04/22 17:23:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.9 MB)\n",
      "16/04/22 17:23:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (PythonRDD[119] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:23:34 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks\n",
      "16/04/22 17:23:34 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 35, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:23:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:23:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.3.108:54340\n",
      "16/04/22 17:23:34 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 35) in 242 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:23:34 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:23:34 INFO DAGScheduler: ResultStage 47 (runJob at PythonRDD.scala:361) finished in 0.246 s\n",
      "16/04/22 17:23:34 INFO DAGScheduler: Job 23 finished: runJob at PythonRDD.scala:361, took 0.299811 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:23:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:23:34 INFO JobScheduler: Finished job streaming job 1461345810000 ms.0 from job set of time 1461345810000 ms\n",
      "16/04/22 17:23:34 INFO PythonRDD: Removing RDD 107 from persistence list\n",
      "16/04/22 17:23:34 INFO JobScheduler: Total delay: 4.657 s for time 1461345810000 ms (execution: 0.927 s)\n",
      "16/04/22 17:23:34 INFO BlockManager: Removing RDD 107\n",
      "16/04/22 17:23:34 INFO PythonRDD: Removing RDD 102 from persistence list\n",
      "16/04/22 17:23:34 INFO BlockManager: Removing RDD 102\n",
      "16/04/22 17:23:34 INFO PythonRDD: Removing RDD 101 from persistence list\n",
      "16/04/22 17:23:34 INFO BlockManager: Removing RDD 101\n",
      "16/04/22 17:23:34 INFO KafkaRDD: Removing RDD 100 from persistence list\n",
      "16/04/22 17:23:34 INFO BlockManager: Removing RDD 100\n",
      "16/04/22 17:23:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:23:34 INFO InputInfoTracker: remove old batch metadata: 1461345750000 ms\n",
      "16/04/22 17:24:00 INFO JobScheduler: Added jobs for time 1461345840000 ms\n",
      "16/04/22 17:24:00 INFO JobScheduler: Starting job streaming job 1461345840000 ms.0 from job set of time 1461345840000 ms\n",
      "16/04/22 17:24:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Registering RDD 124 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Got job 24 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Final stage: ResultStage 49(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 48)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting ShuffleMapStage 48 (PairwiseRDD[124] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=392273, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 11.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(6056) called with curMem=404249, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:00 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 48 (PairwiseRDD[124] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 36, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 36) in 226 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:00 INFO DAGScheduler: ShuffleMapStage 48 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.228 s\n",
      "16/04/22 17:24:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:24:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:24:00 INFO DAGScheduler: waiting: Set(ResultStage 49)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Missing parents for ResultStage 49: List()\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting ResultStage 49 (PythonRDD[128] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=410305, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 6.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=417161, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:00 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (PythonRDD[128] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 37, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.3.108:54340\n",
      "16/04/22 17:24:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 139 bytes\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 37) in 188 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: ResultStage 49 (runJob at PythonRDD.scala:361) finished in 0.191 s\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:00 INFO DAGScheduler: Job 24 finished: runJob at PythonRDD.scala:361, took 0.529626 s\n",
      "16/04/22 17:24:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Got job 25 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Final stage: ResultStage 51(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting ResultStage 51 (PythonRDD[129] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=420903, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 6.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=427759, maxMem=556038881\n",
      "16/04/22 17:24:00 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:00 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (PythonRDD[129] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 38, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:24:00 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:00 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 38) in 178 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:00 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:00 INFO DAGScheduler: ResultStage 51 (runJob at PythonRDD.scala:361) finished in 0.186 s\n",
      "16/04/22 17:24:00 INFO DAGScheduler: Job 25 finished: runJob at PythonRDD.scala:361, took 0.239784 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:24:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:24:00 INFO JobScheduler: Finished job streaming job 1461345840000 ms.0 from job set of time 1461345840000 ms\n",
      "16/04/22 17:24:00 INFO JobScheduler: Total delay: 0.990 s for time 1461345840000 ms (execution: 0.846 s)\n",
      "16/04/22 17:24:00 INFO PythonRDD: Removing RDD 117 from persistence list\n",
      "16/04/22 17:24:00 INFO BlockManager: Removing RDD 117\n",
      "16/04/22 17:24:01 INFO PythonRDD: Removing RDD 112 from persistence list\n",
      "16/04/22 17:24:01 INFO BlockManager: Removing RDD 112\n",
      "16/04/22 17:24:01 INFO PythonRDD: Removing RDD 111 from persistence list\n",
      "16/04/22 17:24:01 INFO BlockManager: Removing RDD 111\n",
      "16/04/22 17:24:01 INFO KafkaRDD: Removing RDD 110 from persistence list\n",
      "16/04/22 17:24:01 INFO BlockManager: Removing RDD 110\n",
      "16/04/22 17:24:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:24:01 INFO InputInfoTracker: remove old batch metadata: 1461345780000 ms\n",
      "16/04/22 17:24:30 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:24:33 INFO JobScheduler: Added jobs for time 1461345870000 ms\n",
      "16/04/22 17:24:33 INFO JobScheduler: Starting job streaming job 1461345870000 ms.0 from job set of time 1461345870000 ms\n",
      "16/04/22 17:24:33 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Registering RDD 134 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Got job 26 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Final stage: ResultStage 53(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 52)\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Submitting ShuffleMapStage 52 (PairwiseRDD[134] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:24:33 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=431501, maxMem=556038881\n",
      "16/04/22 17:24:33 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 11.7 KB, free 529.9 MB)\n",
      "16/04/22 17:24:33 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=443477, maxMem=556038881\n",
      "16/04/22 17:24:33 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.9 MB)\n",
      "16/04/22 17:24:33 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:33 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 52 (PairwiseRDD[134] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:24:33 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks\n",
      "16/04/22 17:24:33 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 39, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:24:33 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:34 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 39) in 254 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:34 INFO DAGScheduler: ShuffleMapStage 52 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.259 s\n",
      "16/04/22 17:24:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:24:34 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:24:34 INFO DAGScheduler: waiting: Set(ResultStage 53)\n",
      "16/04/22 17:24:34 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:24:34 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:34 INFO DAGScheduler: Missing parents for ResultStage 53: List()\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Submitting ResultStage 53 (PythonRDD[138] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:24:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=449535, maxMem=556038881\n",
      "16/04/22 17:24:34 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:24:34 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=456391, maxMem=556038881\n",
      "16/04/22 17:24:34 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:24:34 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:34 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (PythonRDD[138] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:24:34 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks\n",
      "16/04/22 17:24:34 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 40, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:24:34 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.3.108:54340\n",
      "16/04/22 17:24:34 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 139 bytes\n",
      "16/04/22 17:24:34 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 40) in 169 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:34 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:34 INFO DAGScheduler: ResultStage 53 (runJob at PythonRDD.scala:361) finished in 0.171 s\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Job 26 finished: runJob at PythonRDD.scala:361, took 0.563004 s\n",
      "16/04/22 17:24:34 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Got job 27 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Final stage: ResultStage 55(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Submitting ResultStage 55 (PythonRDD[139] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:24:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=460133, maxMem=556038881\n",
      "16/04/22 17:24:34 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:24:34 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=466989, maxMem=556038881\n",
      "16/04/22 17:24:34 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:24:34 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:24:34 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (PythonRDD[139] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:24:34 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks\n",
      "16/04/22 17:24:34 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 41, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:24:34 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:24:34 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 41) in 175 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:24:34 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:24:34 INFO DAGScheduler: ResultStage 55 (runJob at PythonRDD.scala:361) finished in 0.178 s\n",
      "16/04/22 17:24:34 INFO DAGScheduler: Job 27 finished: runJob at PythonRDD.scala:361, took 0.244487 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:24:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:24:34 INFO JobScheduler: Finished job streaming job 1461345870000 ms.0 from job set of time 1461345870000 ms\n",
      "16/04/22 17:24:34 INFO JobScheduler: Total delay: 4.678 s for time 1461345870000 ms (execution: 0.911 s)\n",
      "16/04/22 17:24:34 INFO PythonRDD: Removing RDD 127 from persistence list\n",
      "16/04/22 17:24:34 INFO BlockManager: Removing RDD 127\n",
      "16/04/22 17:24:34 INFO PythonRDD: Removing RDD 122 from persistence list\n",
      "16/04/22 17:24:34 INFO BlockManager: Removing RDD 122\n",
      "16/04/22 17:24:34 INFO PythonRDD: Removing RDD 121 from persistence list\n",
      "16/04/22 17:24:34 INFO BlockManager: Removing RDD 121\n",
      "16/04/22 17:24:34 INFO KafkaRDD: Removing RDD 120 from persistence list\n",
      "16/04/22 17:24:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:24:34 INFO BlockManager: Removing RDD 120\n",
      "16/04/22 17:24:34 INFO InputInfoTracker: remove old batch metadata: 1461345810000 ms\n",
      "16/04/22 17:25:00 INFO JobScheduler: Starting job streaming job 1461345900000 ms.0 from job set of time 1461345900000 ms\n",
      "16/04/22 17:25:00 INFO JobScheduler: Added jobs for time 1461345900000 ms\n",
      "16/04/22 17:25:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Registering RDD 144 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Got job 28 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Final stage: ResultStage 57(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 56)\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 56)\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Submitting ShuffleMapStage 56 (PairwiseRDD[144] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:25:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=470731, maxMem=556038881\n",
      "16/04/22 17:25:00 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 11.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:00 INFO MemoryStore: ensureFreeSpace(6055) called with curMem=482707, maxMem=556038881\n",
      "16/04/22 17:25:00 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.8 MB)\n",
      "16/04/22 17:25:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:00 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (PairwiseRDD[144] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:25:00 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks\n",
      "16/04/22 17:25:00 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 42, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:25:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:00 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 42) in 410 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:00 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:00 INFO DAGScheduler: ShuffleMapStage 56 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.414 s\n",
      "16/04/22 17:25:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:25:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:25:00 INFO DAGScheduler: waiting: Set(ResultStage 57)\n",
      "16/04/22 17:25:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Missing parents for ResultStage 57: List()\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Submitting ResultStage 57 (PythonRDD[148] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:25:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=488762, maxMem=556038881\n",
      "16/04/22 17:25:00 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:00 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=495618, maxMem=556038881\n",
      "16/04/22 17:25:00 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:00 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (PythonRDD[148] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:25:00 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks\n",
      "16/04/22 17:25:00 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 43, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:25:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.3.100:40626\n",
      "16/04/22 17:25:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 139 bytes\n",
      "16/04/22 17:25:00 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 43) in 215 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:00 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:00 INFO DAGScheduler: ResultStage 57 (runJob at PythonRDD.scala:361) finished in 0.223 s\n",
      "16/04/22 17:25:00 INFO DAGScheduler: Job 28 finished: runJob at PythonRDD.scala:361, took 0.763488 s\n",
      "16/04/22 17:25:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Got job 29 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Final stage: ResultStage 59(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Submitting ResultStage 59 (PythonRDD[149] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:25:01 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=499360, maxMem=556038881\n",
      "16/04/22 17:25:01 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:01 INFO MemoryStore: ensureFreeSpace(3742) called with curMem=506216, maxMem=556038881\n",
      "16/04/22 17:25:01 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:01 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:01 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (PythonRDD[149] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:25:01 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks\n",
      "16/04/22 17:25:01 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 44, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:25:01 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:01 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 44) in 174 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:01 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:01 INFO DAGScheduler: ResultStage 59 (runJob at PythonRDD.scala:361) finished in 0.178 s\n",
      "16/04/22 17:25:01 INFO DAGScheduler: Job 29 finished: runJob at PythonRDD.scala:361, took 0.230362 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:25:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:25:01 INFO JobScheduler: Finished job streaming job 1461345900000 ms.0 from job set of time 1461345900000 ms\n",
      "16/04/22 17:25:01 INFO JobScheduler: Total delay: 1.300 s for time 1461345900000 ms (execution: 1.138 s)\n",
      "16/04/22 17:25:01 INFO PythonRDD: Removing RDD 137 from persistence list\n",
      "16/04/22 17:25:01 INFO BlockManager: Removing RDD 137\n",
      "16/04/22 17:25:01 INFO PythonRDD: Removing RDD 132 from persistence list\n",
      "16/04/22 17:25:01 INFO BlockManager: Removing RDD 132\n",
      "16/04/22 17:25:01 INFO PythonRDD: Removing RDD 131 from persistence list\n",
      "16/04/22 17:25:01 INFO BlockManager: Removing RDD 131\n",
      "16/04/22 17:25:01 INFO KafkaRDD: Removing RDD 130 from persistence list\n",
      "16/04/22 17:25:01 INFO BlockManager: Removing RDD 130\n",
      "16/04/22 17:25:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:25:01 INFO InputInfoTracker: remove old batch metadata: 1461345840000 ms\n",
      "16/04/22 17:25:30 INFO JobScheduler: Added jobs for time 1461345930000 ms\n",
      "16/04/22 17:25:30 INFO JobScheduler: Starting job streaming job 1461345930000 ms.0 from job set of time 1461345930000 ms\n",
      "16/04/22 17:25:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Registering RDD 154 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Got job 30 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Final stage: ResultStage 61(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 60)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 60)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Submitting ShuffleMapStage 60 (PairwiseRDD[154] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:25:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=509958, maxMem=556038881\n",
      "16/04/22 17:25:30 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 11.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:30 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=521934, maxMem=556038881\n",
      "16/04/22 17:25:30 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.8 MB)\n",
      "16/04/22 17:25:30 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:30 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (PairwiseRDD[154] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:25:30 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks\n",
      "16/04/22 17:25:30 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 45, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:25:30 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:30 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 45) in 300 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:30 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:30 INFO DAGScheduler: ShuffleMapStage 60 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.306 s\n",
      "16/04/22 17:25:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:25:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:25:30 INFO DAGScheduler: waiting: Set(ResultStage 61)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Missing parents for ResultStage 61: List()\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Submitting ResultStage 61 (PythonRDD[158] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:25:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=527992, maxMem=556038881\n",
      "16/04/22 17:25:30 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:30 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=534848, maxMem=556038881\n",
      "16/04/22 17:25:30 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:30 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:30 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (PythonRDD[158] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:25:30 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks\n",
      "16/04/22 17:25:30 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 46, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:25:30 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.3.100:40626\n",
      "16/04/22 17:25:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 139 bytes\n",
      "16/04/22 17:25:30 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 46) in 163 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:30 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:30 INFO DAGScheduler: ResultStage 61 (runJob at PythonRDD.scala:361) finished in 0.166 s\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Job 30 finished: runJob at PythonRDD.scala:361, took 0.567458 s\n",
      "16/04/22 17:25:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Got job 31 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Final stage: ResultStage 63(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:25:30 INFO DAGScheduler: Submitting ResultStage 63 (PythonRDD[159] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:25:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=538589, maxMem=556038881\n",
      "16/04/22 17:25:30 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 6.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:31 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=545445, maxMem=556038881\n",
      "16/04/22 17:25:31 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.8 MB)\n",
      "16/04/22 17:25:31 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:25:31 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:25:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (PythonRDD[159] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:25:31 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks\n",
      "16/04/22 17:25:31 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 47, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:25:31 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:25:31 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 47) in 167 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:25:31 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:25:31 INFO DAGScheduler: ResultStage 63 (runJob at PythonRDD.scala:361) finished in 0.168 s\n",
      "16/04/22 17:25:31 INFO DAGScheduler: Job 31 finished: runJob at PythonRDD.scala:361, took 0.392883 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:25:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:25:31 INFO JobScheduler: Finished job streaming job 1461345930000 ms.0 from job set of time 1461345930000 ms\n",
      "16/04/22 17:25:31 INFO JobScheduler: Total delay: 1.246 s for time 1461345930000 ms (execution: 1.076 s)\n",
      "16/04/22 17:25:31 INFO PythonRDD: Removing RDD 147 from persistence list\n",
      "16/04/22 17:25:31 INFO BlockManager: Removing RDD 147\n",
      "16/04/22 17:25:31 INFO PythonRDD: Removing RDD 142 from persistence list\n",
      "16/04/22 17:25:31 INFO PythonRDD: Removing RDD 141 from persistence list\n",
      "16/04/22 17:25:31 INFO BlockManager: Removing RDD 142\n",
      "16/04/22 17:25:31 INFO BlockManager: Removing RDD 141\n",
      "16/04/22 17:25:31 INFO KafkaRDD: Removing RDD 140 from persistence list\n",
      "16/04/22 17:25:31 INFO BlockManager: Removing RDD 140\n",
      "16/04/22 17:25:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:25:31 INFO InputInfoTracker: remove old batch metadata: 1461345870000 ms\n",
      "16/04/22 17:26:00 INFO JobScheduler: Added jobs for time 1461345960000 ms\n",
      "16/04/22 17:26:00 INFO JobScheduler: Starting job streaming job 1461345960000 ms.0 from job set of time 1461345960000 ms\n",
      "16/04/22 17:26:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Registering RDD 164 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Got job 32 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Final stage: ResultStage 65(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 64)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting ShuffleMapStage 64 (PairwiseRDD[164] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=549186, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 11.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=561162, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:00 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 64 (PairwiseRDD[164] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:26:00 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks\n",
      "16/04/22 17:26:00 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 48, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:00 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 48) in 225 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: ShuffleMapStage 64 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.225 s\n",
      "16/04/22 17:26:00 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:26:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:26:00 INFO DAGScheduler: waiting: Set(ResultStage 65)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Missing parents for ResultStage 65: List()\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting ResultStage 65 (PythonRDD[168] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=567220, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 6.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=574076, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:00 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (PythonRDD[168] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:26:00 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks\n",
      "16/04/22 17:26:00 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 49, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.3.108:54340\n",
      "16/04/22 17:26:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 139 bytes\n",
      "16/04/22 17:26:00 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 49) in 271 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:26:00 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:00 INFO DAGScheduler: ResultStage 65 (runJob at PythonRDD.scala:361) finished in 0.276 s\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Job 32 finished: runJob at PythonRDD.scala:361, took 0.591540 s\n",
      "16/04/22 17:26:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Got job 33 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Final stage: ResultStage 67(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting ResultStage 67 (PythonRDD[169] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=577817, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 6.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO MemoryStore: ensureFreeSpace(3741) called with curMem=584673, maxMem=556038881\n",
      "16/04/22 17:26:00 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:00 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (PythonRDD[169] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:26:00 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks\n",
      "16/04/22 17:26:00 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 50, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:26:00 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:01 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 50) in 149 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:26:01 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:01 INFO DAGScheduler: ResultStage 67 (runJob at PythonRDD.scala:361) finished in 0.155 s\n",
      "16/04/22 17:26:01 INFO DAGScheduler: Job 33 finished: runJob at PythonRDD.scala:361, took 0.212190 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:26:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:26:01 INFO JobScheduler: Finished job streaming job 1461345960000 ms.0 from job set of time 1461345960000 ms\n",
      "16/04/22 17:26:01 INFO JobScheduler: Total delay: 1.066 s for time 1461345960000 ms (execution: 0.896 s)\n",
      "16/04/22 17:26:01 INFO PythonRDD: Removing RDD 157 from persistence list\n",
      "16/04/22 17:26:01 INFO PythonRDD: Removing RDD 152 from persistence list\n",
      "16/04/22 17:26:01 INFO BlockManager: Removing RDD 157\n",
      "16/04/22 17:26:01 INFO BlockManager: Removing RDD 152\n",
      "16/04/22 17:26:01 INFO PythonRDD: Removing RDD 151 from persistence list\n",
      "16/04/22 17:26:01 INFO KafkaRDD: Removing RDD 150 from persistence list\n",
      "16/04/22 17:26:01 INFO BlockManager: Removing RDD 151\n",
      "16/04/22 17:26:01 INFO BlockManager: Removing RDD 150\n",
      "16/04/22 17:26:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:26:01 INFO InputInfoTracker: remove old batch metadata: 1461345900000 ms\n",
      "16/04/22 17:26:30 INFO JobScheduler: Added jobs for time 1461345990000 ms\n",
      "16/04/22 17:26:30 INFO JobScheduler: Starting job streaming job 1461345990000 ms.0 from job set of time 1461345990000 ms\n",
      "16/04/22 17:26:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Registering RDD 174 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Got job 34 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Final stage: ResultStage 69(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 68)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 68)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting ShuffleMapStage 68 (PairwiseRDD[174] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=588414, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 11.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=600390, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:30 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 68 (PairwiseRDD[174] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 51, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 51) in 216 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:30 INFO DAGScheduler: ShuffleMapStage 68 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.219 s\n",
      "16/04/22 17:26:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:26:30 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:26:30 INFO DAGScheduler: waiting: Set(ResultStage 69)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Missing parents for ResultStage 69: List()\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting ResultStage 69 (PythonRDD[178] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=606448, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 6.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=613304, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:30 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (PythonRDD[178] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 52, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.3.100:40626\n",
      "16/04/22 17:26:30 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 139 bytes\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 52) in 158 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:30 INFO DAGScheduler: ResultStage 69 (runJob at PythonRDD.scala:361) finished in 0.161 s\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Job 34 finished: runJob at PythonRDD.scala:361, took 0.461904 s\n",
      "16/04/22 17:26:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Got job 35 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Final stage: ResultStage 71(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting ResultStage 71 (PythonRDD[179] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=617047, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 6.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=623903, maxMem=556038881\n",
      "16/04/22 17:26:30 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.7 MB)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:26:30 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (PythonRDD[179] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 53, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:26:30 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:26:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.3.108:54340\n",
      "16/04/22 17:26:30 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 53) in 195 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:26:30 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:26:30 INFO DAGScheduler: ResultStage 71 (runJob at PythonRDD.scala:361) finished in 0.199 s\n",
      "16/04/22 17:26:30 INFO DAGScheduler: Job 35 finished: runJob at PythonRDD.scala:361, took 0.258713 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:26:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:26:30 INFO JobScheduler: Finished job streaming job 1461345990000 ms.0 from job set of time 1461345990000 ms\n",
      "16/04/22 17:26:30 INFO PythonRDD: Removing RDD 167 from persistence list\n",
      "16/04/22 17:26:30 INFO JobScheduler: Total delay: 0.943 s for time 1461345990000 ms (execution: 0.804 s)\n",
      "16/04/22 17:26:30 INFO PythonRDD: Removing RDD 162 from persistence list\n",
      "16/04/22 17:26:30 INFO BlockManager: Removing RDD 167\n",
      "16/04/22 17:26:30 INFO BlockManager: Removing RDD 162\n",
      "16/04/22 17:26:30 INFO PythonRDD: Removing RDD 161 from persistence list\n",
      "16/04/22 17:26:30 INFO KafkaRDD: Removing RDD 160 from persistence list\n",
      "16/04/22 17:26:30 INFO BlockManager: Removing RDD 161\n",
      "16/04/22 17:26:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:26:30 INFO BlockManager: Removing RDD 160\n",
      "16/04/22 17:26:30 INFO InputInfoTracker: remove old batch metadata: 1461345930000 ms\n",
      "16/04/22 17:27:00 INFO JobScheduler: Added jobs for time 1461346020000 ms\n",
      "16/04/22 17:27:00 INFO JobScheduler: Starting job streaming job 1461346020000 ms.0 from job set of time 1461346020000 ms\n",
      "16/04/22 17:27:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Registering RDD 184 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Got job 36 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Final stage: ResultStage 73(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 72)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting ShuffleMapStage 72 (PairwiseRDD[184] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=627646, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 11.7 KB, free 529.7 MB)\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=639622, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.7 MB)\n",
      "16/04/22 17:27:00 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:27:00 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (PairwiseRDD[184] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:27:00 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks\n",
      "16/04/22 17:27:00 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 54, 10.0.3.108, ANY, 3587 bytes)\n",
      "16/04/22 17:27:00 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.3.108:57281 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:00 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 54) in 339 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:27:00 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:00 INFO DAGScheduler: ShuffleMapStage 72 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.344 s\n",
      "16/04/22 17:27:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:27:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:27:00 INFO DAGScheduler: waiting: Set(ResultStage 73)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Missing parents for ResultStage 73: List()\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting ResultStage 73 (PythonRDD[188] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=645680, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 6.7 KB, free 529.7 MB)\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=652536, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.7 MB)\n",
      "16/04/22 17:27:00 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:27:00 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (PythonRDD[188] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:27:00 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks\n",
      "16/04/22 17:27:00 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 55, 10.0.3.100, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:27:00 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.3.100:52291 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.3.100:40626\n",
      "16/04/22 17:27:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 139 bytes\n",
      "16/04/22 17:27:00 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 55) in 182 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:27:00 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:00 INFO DAGScheduler: ResultStage 73 (runJob at PythonRDD.scala:361) finished in 0.185 s\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Job 36 finished: runJob at PythonRDD.scala:361, took 0.641693 s\n",
      "16/04/22 17:27:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Got job 37 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Final stage: ResultStage 75(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 74)\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting ResultStage 75 (PythonRDD[189] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=656279, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 6.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:00 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=663135, maxMem=556038881\n",
      "16/04/22 17:27:00 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:00 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:27:00 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (PythonRDD[189] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:27:00 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks\n",
      "16/04/22 17:27:00 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 56, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:27:01 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.3.108:54340\n",
      "16/04/22 17:27:01 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 56) in 183 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:27:01 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:01 INFO DAGScheduler: ResultStage 75 (runJob at PythonRDD.scala:361) finished in 0.187 s\n",
      "16/04/22 17:27:01 INFO DAGScheduler: Job 37 finished: runJob at PythonRDD.scala:361, took 0.253979 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:27:00\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:27:01 INFO JobScheduler: Finished job streaming job 1461346020000 ms.0 from job set of time 1461346020000 ms\n",
      "16/04/22 17:27:01 INFO JobScheduler: Total delay: 1.154 s for time 1461346020000 ms (execution: 1.005 s)\n",
      "16/04/22 17:27:01 INFO PythonRDD: Removing RDD 177 from persistence list\n",
      "16/04/22 17:27:01 INFO BlockManager: Removing RDD 177\n",
      "16/04/22 17:27:01 INFO PythonRDD: Removing RDD 172 from persistence list\n",
      "16/04/22 17:27:01 INFO BlockManager: Removing RDD 172\n",
      "16/04/22 17:27:01 INFO PythonRDD: Removing RDD 171 from persistence list\n",
      "16/04/22 17:27:01 INFO KafkaRDD: Removing RDD 170 from persistence list\n",
      "16/04/22 17:27:01 INFO BlockManager: Removing RDD 171\n",
      "16/04/22 17:27:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:27:01 INFO BlockManager: Removing RDD 170\n",
      "16/04/22 17:27:01 INFO InputInfoTracker: remove old batch metadata: 1461345960000 ms\n",
      "16/04/22 17:27:30 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "16/04/22 17:27:33 INFO JobScheduler: Added jobs for time 1461346050000 ms\n",
      "16/04/22 17:27:33 INFO JobScheduler: Starting job streaming job 1461346050000 ms.0 from job set of time 1461346050000 ms\n",
      "16/04/22 17:27:33 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Registering RDD 194 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Got job 38 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Final stage: ResultStage 77(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 76)\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Submitting ShuffleMapStage 76 (PairwiseRDD[194] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:27:33 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=666878, maxMem=556038881\n",
      "16/04/22 17:27:33 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 11.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:33 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=678854, maxMem=556038881\n",
      "16/04/22 17:27:33 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.6 MB)\n",
      "16/04/22 17:27:33 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.1 MB)\n",
      "16/04/22 17:27:33 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 76 (PairwiseRDD[194] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:27:33 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks\n",
      "16/04/22 17:27:33 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 57, 10.0.3.100, ANY, 3587 bytes)\n",
      "16/04/22 17:27:33 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:34 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 57) in 247 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:27:34 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:34 INFO DAGScheduler: ShuffleMapStage 76 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.249 s\n",
      "16/04/22 17:27:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:27:34 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:27:34 INFO DAGScheduler: waiting: Set(ResultStage 77)\n",
      "16/04/22 17:27:34 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Missing parents for ResultStage 77: List()\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Submitting ResultStage 77 (PythonRDD[198] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "16/04/22 17:27:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=684912, maxMem=556038881\n",
      "16/04/22 17:27:34 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 6.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:34 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=691768, maxMem=556038881\n",
      "16/04/22 17:27:34 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:34 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.1 MB)\n",
      "16/04/22 17:27:34 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (PythonRDD[198] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:27:34 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks\n",
      "16/04/22 17:27:34 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 58, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:27:34 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.3.108:54340\n",
      "16/04/22 17:27:34 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 139 bytes\n",
      "16/04/22 17:27:34 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 58) in 186 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:27:34 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:34 INFO DAGScheduler: ResultStage 77 (runJob at PythonRDD.scala:361) finished in 0.190 s\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Job 38 finished: runJob at PythonRDD.scala:361, took 0.537071 s\n",
      "16/04/22 17:27:34 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Got job 39 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Final stage: ResultStage 79(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Missing parents: List()\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Submitting ResultStage 79 (PythonRDD[199] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "16/04/22 17:27:34 INFO MemoryStore: ensureFreeSpace(6856) called with curMem=695511, maxMem=556038881\n",
      "16/04/22 17:27:34 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 6.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:34 INFO MemoryStore: ensureFreeSpace(3743) called with curMem=702367, maxMem=556038881\n",
      "16/04/22 17:27:34 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.7 KB, free 529.6 MB)\n",
      "16/04/22 17:27:34 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.3.142:45405 (size: 3.7 KB, free: 530.0 MB)\n",
      "16/04/22 17:27:34 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (PythonRDD[199] at RDD at PythonRDD.scala:43)\n",
      "16/04/22 17:27:34 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks\n",
      "16/04/22 17:27:34 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 59, 10.0.3.108, PROCESS_LOCAL, 3476 bytes)\n",
      "16/04/22 17:27:34 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.3.108:57281 (size: 3.7 KB, free: 530.2 MB)\n",
      "16/04/22 17:27:34 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 59) in 198 ms on 10.0.3.108 (1/1)\n",
      "16/04/22 17:27:34 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:27:34 INFO DAGScheduler: ResultStage 79 (runJob at PythonRDD.scala:361) finished in 0.203 s\n",
      "16/04/22 17:27:34 INFO DAGScheduler: Job 39 finished: runJob at PythonRDD.scala:361, took 0.271041 s\n",
      "-------------------------------------------\n",
      "Time: 2016-04-22 17:27:30\n",
      "-------------------------------------------\n",
      "\n",
      "16/04/22 17:27:34 INFO JobScheduler: Finished job streaming job 1461346050000 ms.0 from job set of time 1461346050000 ms\n",
      "16/04/22 17:27:34 INFO JobScheduler: Total delay: 4.693 s for time 1461346050000 ms (execution: 0.916 s)\n",
      "16/04/22 17:27:34 INFO PythonRDD: Removing RDD 187 from persistence list\n",
      "16/04/22 17:27:34 INFO PythonRDD: Removing RDD 182 from persistence list\n",
      "16/04/22 17:27:34 INFO BlockManager: Removing RDD 187\n",
      "16/04/22 17:27:34 INFO PythonRDD: Removing RDD 181 from persistence list\n",
      "16/04/22 17:27:34 INFO BlockManager: Removing RDD 182\n",
      "16/04/22 17:27:34 INFO BlockManager: Removing RDD 181\n",
      "16/04/22 17:27:34 INFO KafkaRDD: Removing RDD 180 from persistence list\n",
      "16/04/22 17:27:34 INFO BlockManager: Removing RDD 180\n",
      "16/04/22 17:27:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "16/04/22 17:27:34 INFO InputInfoTracker: remove old batch metadata: 1461345990000 ms\n",
      "16/04/22 17:28:00 INFO SparkContext: Starting job: runJob at PythonRDD.scala:361\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Registering RDD 204 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Got job 40 (runJob at PythonRDD.scala:361) with 1 output partitions\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Final stage: ResultStage 81(runJob at PythonRDD.scala:361)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 80)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Submitting ShuffleMapStage 80 (PairwiseRDD[204] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "16/04/22 17:28:00 INFO MemoryStore: ensureFreeSpace(11976) called with curMem=706110, maxMem=556038881\n",
      "16/04/22 17:28:00 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 11.7 KB, free 529.6 MB)\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/home/ubuntu/iot-traffic/Prediction/notebooks/timedecaykafka.py\", line 28, in <module>\n",
      "    ssc.awaitTermination()\n",
      "16/04/22 17:28:00 INFO MemoryStore: ensureFreeSpace(6058) called with curMem=718086, maxMem=556038881\n",
      "  File \"/home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/streaming/context.py\", line 246, in awaitTermination\n",
      "  File \"/home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "  File \"/home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "  File \"/home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 473, in send_command\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 430, in readline\n",
      "    data = recv(1)\n",
      "16/04/22 17:28:00 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 5.9 KB, free 529.6 MB)\n",
      "KeyboardInterrupt\n",
      "16/04/22 17:28:00 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.3.142:45405 (size: 5.9 KB, free: 530.0 MB)\n",
      "16/04/22 17:28:00 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:861\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 80 (PairwiseRDD[204] at call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "16/04/22 17:28:00 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks\n",
      "16/04/22 17:28:00 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 60, 10.0.3.100, ANY, 3587 bytes)\n",
      "\n",
      "16/04/22 17:28:00 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook\n",
      "16/04/22 17:28:00 INFO JobGenerator: Stopping JobGenerator immediately\n",
      "16/04/22 17:28:00 INFO RecurringTimer: Stopped timer for JobGenerator after time 1461346080000\n",
      "16/04/22 17:28:00 INFO JobGenerator: Stopped JobGenerator\n",
      "16/04/22 17:28:00 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.3.100:52291 (size: 5.9 KB, free: 530.2 MB)\n",
      "16/04/22 17:28:00 INFO JobScheduler: Stopped JobScheduler\n",
      "16/04/22 17:28:00 INFO StreamingContext: StreamingContext stopped successfully\n",
      "16/04/22 17:28:00 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "16/04/22 17:28:00 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 60) in 207 ms on 10.0.3.100 (1/1)\n",
      "16/04/22 17:28:00 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "16/04/22 17:28:00 ERROR DAGScheduler: Failed to update accumulators for ShuffleMapTask(80, 0)\n",
      "org.apache.spark.SparkException: EOF reached before Python server acknowledged\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:830)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:788)\n",
      "\tat org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:100)\n",
      "\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:351)\n",
      "\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:346)\n",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n",
      "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n",
      "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n",
      "\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n",
      "\tat org.apache.spark.Accumulators$.add(Accumulators.scala:346)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:934)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1030)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1490)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: ShuffleMapStage 80 (call at /home/ubuntu/spark/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.221 s\n",
      "16/04/22 17:28:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/04/22 17:28:00 INFO DAGScheduler: running: Set()\n",
      "16/04/22 17:28:00 INFO DAGScheduler: waiting: Set(ResultStage 81)\n",
      "16/04/22 17:28:00 INFO DAGScheduler: failed: Set()\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Missing parents for ResultStage 81: List()\n",
      "16/04/22 17:28:00 INFO DAGScheduler: Submitting ResultStage 81 (PythonRDD[208] at RDD at PythonRDD.scala:43), which is now runnable\n"
     ]
    }
   ],
   "source": [
    "!../../../spark/spark-1.5.0-bin-hadoop2.6/bin/spark-submit --master spark://10.0.3.70:7077 \\\n",
    "        --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 timedecaykafka.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
